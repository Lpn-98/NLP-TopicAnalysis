{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import os\n",
    "import time\n",
    "import jieba  #处理中文\n",
    "import numpy as np\n",
    "from progressbar import *\n",
    "start1 =time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obatin_news_from_csv(path_csv):\n",
    "    old_text=pd.read_csv(path_csv)\n",
    "    drop_index_na=[]\n",
    "    for i in range(old_text.shape[0]):\n",
    "        j=old_text['微博正文'][i]\n",
    "        if (\"抱歉\" in j or len(j)<40):\n",
    "            drop_index_na.append(i)\n",
    "    text=old_text.drop(drop_index_na)\n",
    "    text=text.reset_index(drop=True)\n",
    "    return text\n",
    "\n",
    "import re\n",
    "def find_title(text):\n",
    "    text[\"标题\"]=[-99 for i in range(text.shape[0])]\n",
    "    text[\"微博正文（去掉标题）\"]=[-99 for i in range(text.shape[0])]\n",
    "    notitle_index=[]\n",
    "    title_p=re.compile(r\".*\\【.*\\】\")\n",
    "    progress = ProgressBar()\n",
    "    for i in progress(range(text.shape[0])):\n",
    "        title_=re.match(title_p,text[\"微博正文\"][i])\n",
    "        if title_:\n",
    "            text[\"标题\"][i]=title_[0]\n",
    "            text[\"微博正文（去掉标题）\"][i]=re.sub(r\".*\\【.*\\】\",\"\",text[\"微博正文\"][i])\n",
    "        else:\n",
    "            notitle_index.append(i)\n",
    "        time.sleep(0.1)\n",
    "    text=text.drop(notitle_index)\n",
    "    text=text.reset_index(drop=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入词典和停用词\n",
    "jieba.load_userdict(r\".\\自定义词典.txt\")\n",
    "stopwords = [line.strip() for line in open(r\".\\停用词库.txt\",encoding='UTF-8').readlines()]\n",
    "#清理内容\n",
    "def clean_text_content(text_content,cut_all=False):\n",
    "    text_content = re.sub(r\"。\", \".\", text_content)\n",
    "    text_content = re.sub(r\"，\", \",\", text_content)\n",
    "    text_content = re.sub(r\"“\", \"'\", text_content)\n",
    "    text_content = re.sub(r\"”\", \"'\", text_content)\n",
    "    text_content = re.sub(r\"…\", \".\", text_content)\n",
    "    text_content = re.sub(r\"@\", \"@\", text_content)\n",
    "    text_content = re.sub(r\" \", \" \", text_content)\n",
    "    text_content = re.sub(r\"！\", \"!\", text_content)\n",
    "    text_content = re.sub(r\"？\", \"?\", text_content)\n",
    "    text_content = re.sub(r\"：\", \":\", text_content)\n",
    "    text_content = re.sub(r\"）\", \")\", text_content)\n",
    "    text_content = re.sub(r\"（\", \"(\", text_content)\n",
    "    text_content = re.sub(r\"(\\d+年)*(\\d+月)*(\\d+[日])\", \"\", text_content) #日期\n",
    "    text_content = re.sub(r\"\\d+[年月日天号人名时例名省市区县院]\", \"\", text_content) \n",
    "    text_content = re.sub(r\"[第]*[零一二三四五六七八九百千万]+[年月日天号人名时例名省市区县院名例周月年]*\", \"\", text_content) \n",
    "    text_content = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text_content) #时间\n",
    "    text_content = re.sub(r\"^[-+]?[0-9]+(\\.)?[0-9]*$\", \"\", text_content) #数字\n",
    "    text_content = re.sub(r\"@\\S*\\:+\\s*\", \" \", text_content) #@小央视频\n",
    "    text_content = re.sub(r\"\\[.+\\]\", \"\", text_content) #[组图共2张]和[加油]\n",
    "    text_content = re.sub(r\"\\(.*?\\)\", \"\", text_content) #（环球网）\n",
    "    \n",
    "    text_content = re.sub(r\"\\W+\\w*(视频)\\s+\", \"\", text_content) #小央视频的秒拍视频\n",
    "    #de这个bug花费我一晚上时间！！因为一开始写\\W+\\S*(视频)\\s+的话，\\S会把,.等也匹配进去，就变成从第一个.匹配了。\n",
    "    #但是\\w是\"匹配特殊字符，即非字母、非数字、非汉字、非_\"，即不会匹配,.\n",
    "    #以一下几个做测试：\n",
    "    #\"戳视频↓星辰浩瀚，2020，我们共同努力！新华视点的秒拍视频 @新华视点  \"\n",
    "    #\"祝你们每天好“星晴”@iPanda熊猫频道 iPanda熊猫频道的微博视频  \"\n",
    "    #\"据悉。日本的独居户已超过三分之一。预计在2040年将达到40%。（全球视频大魔王）全球视频大魔王的微博视频    \"\n",
    "\n",
    "    text_content = re.sub(r\"#\", \"\", text_content) ##\n",
    "    text_content = re.sub(r\"(http|https)(://t.cn/)[a-zA-Z0-9]+\", \"\", text_content) #网址(微博上的连接都是http://t.cn/.....形式)\n",
    "    text_content = re.sub(r\"转发理由:\", \"\", text_content) \n",
    "    text_content = re.sub(r\"转发内容:\", \"\", text_content) \n",
    "    text_content = re.sub(r\"原始用户:.*\", \"\", text_content) \n",
    "    \n",
    "    word_list=jieba.lcut(text_content,cut_all=cut_all)\n",
    "    word_list_len=len(word_list)\n",
    "    i=0\n",
    "    while i<word_list_len:\n",
    "        j=word_list[i]\n",
    "        if j in stopwords or not('\\u4e00'<=j<='\\u9fff') or len(j)<2:\n",
    "            word_list.pop(i)\n",
    "            word_list_len=word_list_len-1\n",
    "        elif (j==\"中方\" or j==\"我国\"):\n",
    "            word_list[i]=\"中国\"\n",
    "        elif j==\"美方\":\n",
    "            word_list[i]=\"美国\"\n",
    "        elif j==\"贸易战\":\n",
    "            word_list[i]=\"贸易\" \n",
    "        else:\n",
    "            i=i+1\n",
    "    if \" \" in word_list:\n",
    "        word_list.remove(\" \")\n",
    "    return word_list#text_content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_text=obatin_news_from_csv(\"./数据集1-环球时报/环球时报/1974576991.csv\")\n",
    "text_column=[\"微博正文\",\"发布时间\"]\n",
    "\n",
    "#把2020年的都去掉\n",
    "len_old_text=old_text.shape[0]\n",
    "drop_index=[]\n",
    "old_pattern=re.compile(r\"2020(\\-)[0-9]+(\\-)[0-9]+\")\n",
    "for i in range(0,len_old_text):\n",
    "    if old_pattern.match(old_text[\"发布时间\"][i]):\n",
    "        drop_index.append(i)\n",
    "text=old_text.drop(drop_index)\n",
    "text=text.reset_index(drop=True)\n",
    "text.head(2)\n",
    "\n",
    "text=find_title(text)\n",
    "text_shape_old=text.shape[0]\n",
    "print(text_shape_old)\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.clock()\n",
    "\n",
    "len_text=text.shape[0]\n",
    "\n",
    "text[\"微博正文(无标题切词后)\"]=[-99 for i in range(len_text)]\n",
    "text[\"标题(切词后)\"]=[-99 for i in range(len_text)]\n",
    "\n",
    "drop_na=[]\n",
    "\n",
    "progress = ProgressBar()\n",
    "\n",
    "for u in progress(range(0,len_text)):\n",
    "    cleaned_content=clean_text_content(text[\"微博正文\"][u])\n",
    "    text[\"微博正文(无标题切词后)\"][u]=\" \".join(cleaned_content)\n",
    "\n",
    "    cleaned_title=clean_text_content(text[\"标题\"][u],True)\n",
    "    text[\"标题(切词后)\"][u]=\" \".join(cleaned_title)\n",
    "\n",
    "    if pd.isnull(text.loc[u]).any():\n",
    "        drop_na.append(u)\n",
    "        print(u) \n",
    "    time.sleep(0.1)\n",
    "\n",
    "text=text.drop(drop_na)\n",
    "text=text.reset_index(drop=True)   \n",
    "\n",
    "text[\"微博正文(有标题切词后)\"]=[-99 for i in range(len_text)]\n",
    "progress = ProgressBar()\n",
    "\n",
    "for u in progress(range(0,len_text)):\n",
    "    text[\"微博正文(有标题切词后)\"][u]=text[\"标题(切词后)\"][u]+\" \"+text[\"微博正文(无标题切词后)\"][u]\n",
    "\n",
    "end = time.clock()\n",
    "\n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text_shape_old\",text_shape_old)\n",
    "print(\"text_shape_new\",text.shape[0])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_new=text[[\"发布时间\",\"微博正文\",\"标题\",\"微博正文（去掉标题）\",\"微博正文(无标题切词后)\",\"标题(切词后)\",\"微博正文(有标题切词后)\"]]\n",
    "text_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath='./huanqiu_news_with_title_2.csv'\n",
    "text_new.to_csv(outputpath,sep=',',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "描述性分析（词云+每个月发的微博数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按照词频画wordcloud\n",
    "all_words_space=\" \".join(('%s' %id for id in text[\"微博正文(有标题切词后)\"]))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "wordcloud1 = WordCloud(\n",
    "       # 设置字体，不然会出现口字乱码，文字的路径是电脑的字体一般路径，可以换成别的\n",
    "       font_path=\"C:/Windows/Fonts/simfang.ttf\",\n",
    "       background_color=\"white\",\n",
    "       # mask参数=图片背景，必须要写上，另外有mask参数再设定宽高是无效的\n",
    "       width=800,\n",
    "       height=660,\n",
    "       max_font_size=200).generate(all_words_space)\n",
    "plt.imshow(wordcloud1.recolor(), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "#保存生成的图片\n",
    "wordcloud1.to_file('./wordcloud1.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用jieba.analyse自带的抽取特征词，按照tf-idf画词云\n",
    "import jieba.analyse\n",
    "tag=jieba.analyse.extract_tags(all_words_space, topK=100, withWeight=True, allowPOS=())\n",
    "wc_tfidf ={}\n",
    "for i in tag:\n",
    "    wc_tfidf[i[0]]=i[1]\n",
    "    \n",
    "wordcloud2 = WordCloud(\n",
    "       # 设置字体，不然会出现口字乱码，文字的路径是电脑的字体一般路径，可以换成别的\n",
    "       font_path=\"C:/Windows/Fonts/simfang.ttf\",\n",
    "       background_color=\"white\",\n",
    "       # mask参数=图片背景，必须要写上，另外有mask参数再设定宽高是无效的\n",
    "       width=800,\n",
    "       height=660,\n",
    "       max_font_size=200).generate_from_frequencies(wc_tfidf)\n",
    "plt.imshow(wordcloud2.recolor(), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "#保存生成的图片\n",
    "wordcloud2.to_file('./result2.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全部微博数据-按月份看数量\n",
    "import re\n",
    "time_all={}\n",
    "for i in range(1,13):\n",
    "    month_text=0\n",
    "    if len(str(i))==1:\n",
    "        mon=\"0\"+str(i)\n",
    "    else:\n",
    "        mon=str(i)\n",
    "    month_re='2019-'+mon\n",
    "    \n",
    "    time_list=text[\"发布时间\"]\n",
    "    for j in time_list:\n",
    "        if month_re in j:\n",
    "            month_text=month_text+1\n",
    "    time_all[mon]=month_text\n",
    "print(time_all)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "month_re_list=[]\n",
    "for i in range(1,13):\n",
    "    if len(str(i))==1:\n",
    "        mon=\"0\"+str(i)\n",
    "    else:\n",
    "        mon=str(i)\n",
    "    month_re='2019-'+mon\n",
    "    month_re_list.append(month_re)\n",
    "#print(\"month_re_list\",month_re_list)\n",
    "new_ticks =month_re_list\n",
    "x = [int(i[0]) for i in time_all.items()]\n",
    "y = [int(i[1]) for i in time_all.items()]\n",
    "print(\"x\",x)\n",
    "print(\"y\",y)\n",
    "print(\"month_re_list\",month_re_list)\n",
    "print(\"sum(y)\",sum(y))\n",
    "\n",
    "plt.figure(figsize=(10,8),dpi=50)\n",
    "plt.ylim(0,2000)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.bar(month_re_list ,y,width=0.8)\n",
    "for m,n in zip(x ,y):\n",
    "    plt.text(m-0.7, n+1, '%.0f' % n, ha='right', va= 'bottom',fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
