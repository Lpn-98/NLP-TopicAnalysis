{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import os\n",
    "import time\n",
    "import jieba  #处理中文\n",
    "import numpy as np\n",
    "from progressbar import *\n",
    "start1 =time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obatin_news_from_csv(path_csv):\n",
    "    old_text=pd.read_csv(path_csv)\n",
    "    drop_index_na=[]\n",
    "    for i in range(old_text.shape[0]):\n",
    "        j=old_text['微博正文'][i]\n",
    "        if (\"抱歉\" in j or len(j)<40):\n",
    "            drop_index_na.append(i)\n",
    "    text=old_text.drop(drop_index_na)\n",
    "    text=text.reset_index(drop=True)\n",
    "    return text\n",
    "\n",
    "import re\n",
    "def find_title(text):\n",
    "    text[\"标题\"]=[-99 for i in range(text.shape[0])]\n",
    "    title_p=re.compile(r\".*\\【.*\\】\")\n",
    "    progress = ProgressBar()\n",
    "    for i in progress(range(len(text[\"微博正文\"]))):\n",
    "        title_=re.match(title_p,text[\"微博正文\"][i])\n",
    "        if title_:\n",
    "            text[\"标题\"][i]=title_[0]\n",
    "        text[\"微博正文\"][i]=re.sub(\".*\\【.*\\】\",\"\",text[\"微博正文\"][i])\n",
    "        time.sleep(0.1)\n",
    "    notitle_index=[]\n",
    "    for i in range(text.shape[0]):\n",
    "        if text[\"标题\"][i]==-99:\n",
    "            notitle_index.append(i)\n",
    "    text=text.drop(notitle_index)\n",
    "    text=text.reset_index(drop=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入词典和停用词\n",
    "jieba.load_userdict(r\".\\自定义词典.txt\")\n",
    "stopwords = [line.strip() for line in open(r\".\\停用词库.txt\",encoding='UTF-8').readlines()]\n",
    "#清理内容\n",
    "def clean_text_content(text_content,cut_all=False):\n",
    "    text_content = re.sub(r\"。\", \".\", text_content)\n",
    "    text_content = re.sub(r\"，\", \",\", text_content)\n",
    "    text_content = re.sub(r\"“\", \"'\", text_content)\n",
    "    text_content = re.sub(r\"”\", \"'\", text_content)\n",
    "    text_content = re.sub(r\"…\", \".\", text_content)\n",
    "    text_content = re.sub(r\"@\", \"@\", text_content)\n",
    "    text_content = re.sub(r\" \", \" \", text_content)\n",
    "    text_content = re.sub(r\"！\", \"!\", text_content)\n",
    "    text_content = re.sub(r\"？\", \"?\", text_content)\n",
    "    text_content = re.sub(r\"：\", \":\", text_content)\n",
    "    text_content = re.sub(r\"）\", \")\", text_content)\n",
    "    text_content = re.sub(r\"（\", \"(\", text_content)\n",
    "    text_content = re.sub(r\"(\\d+年)*(\\d+月)*(\\d+[日])\", \"\", text_content) #日期\n",
    "    text_content = re.sub(r\"\\d+[年月日天号人名时例名省市区县院]\", \"\", text_content) \n",
    "    text_content = re.sub(r\"[第]*[零一二三四五六七八九百千万]+[年月日天号人名时例名省市区县院名例周月年]*\", \"\", text_content) \n",
    "    text_content = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text_content) #时间\n",
    "    text_content = re.sub(r\"^[-+]?[0-9]+(\\.)?[0-9]*$\", \"\", text_content) #数字\n",
    "    text_content = re.sub(r\"@\\S*\\s\", \" \", text_content) #@小央视频\n",
    "    text_content = re.sub(r\"\\[.+\\]\", \"\", text_content) #[组图共2张]和[加油]\n",
    "    text_content = re.sub(r\"\\(.*\\)\", \"\", text_content) #（环球网）\n",
    "    \n",
    "    text_content = re.sub(r\"\\W+\\w*(视频)\\s+\", \"\", text_content) #小央视频的秒拍视频\n",
    "    #de这个bug花费我一晚上时间！！因为一开始写\\W+\\S*(视频)\\s+的话，\\S会把,.等也匹配进去，就变成从第一个.匹配了。\n",
    "    #但是\\w是\"匹配特殊字符，即非字母、非数字、非汉字、非_\"，即不会匹配,.\n",
    "    #以一下几个做测试：\n",
    "    #\"戳视频↓星辰浩瀚，2020，我们共同努力！新华视点的秒拍视频 @新华视点  \"\n",
    "    #\"祝你们每天好“星晴”@iPanda熊猫频道 iPanda熊猫频道的微博视频  \"\n",
    "    #\"据悉。日本的独居户已超过三分之一。预计在2040年将达到40%。（全球视频大魔王）全球视频大魔王的微博视频    \"\n",
    "\n",
    "    text_content = re.sub(r\"#\", \"\", text_content) ##\n",
    "    text_content = re.sub(r\"(http|https)(://t.cn/)[a-zA-Z0-9]+\", \"\", text_content) #网址(微博上的连接都是http://t.cn/.....形式)\n",
    "    text_content = re.sub(r\"转发理由:\", \"\", text_content) \n",
    "    text_content = re.sub(r\"转发内容:\", \"\", text_content) \n",
    "    text_content = re.sub(r\"原始用户:.*\", \"\", text_content) \n",
    "    \n",
    "    word_list=jieba.lcut(text_content,cut_all=cut_all)\n",
    "    word_list_len=len(word_list)\n",
    "    i=0\n",
    "    while i<word_list_len:\n",
    "        j=word_list[i]\n",
    "        if j in stopwords or not('\\u4e00'<=j<='\\u9fff') or len(j)<2:\n",
    "            word_list.pop(i)\n",
    "            word_list_len=word_list_len-1\n",
    "        elif (j==\"中方\" or j==\"我国\"):\n",
    "            word_list[i]=\"中国\"\n",
    "        elif j==\"美方\":\n",
    "            word_list[i]=\"美国\"\n",
    "        elif j==\"贸易战\":\n",
    "            word_list[i]=\"贸易\" \n",
    "        else:\n",
    "            i=i+1\n",
    "    if \" \" in word_list:\n",
    "        word_list.remove(\" \")\n",
    "    return word_list#text_content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "a=[\n",
    "    \"./数据集2-五个分类/中国农业新闻网/2106046392.csv\",\n",
    "    \"./数据集2-五个分类/新浪教育/1225314032.csv\",\n",
    "    \"./数据集2-五个分类/新浪财经/1638782947.csv\",\n",
    "    \"./数据集2-五个分类/新华体育/1731169267.csv\",\n",
    "    \"./数据集2-五个分类/新华国际/2641686425.csv\"\n",
    "]\n",
    "\n",
    "text_one_news_list=[]\n",
    "dict_label={\"农业\":1,\"教育\":2,\"财经\":3,\"体育\":4,\"国际\":5}\n",
    "\n",
    "for i in a:\n",
    "    \n",
    "    pattern1=re.compile(r\"[\\u4e00-\\u9fa5]*\")\n",
    "    class_name1=pattern1.findall(i)[7]\n",
    "    class_name=class_name1[len(class_name1)-2:len(class_name1)]\n",
    "    if class_name==\"闻网\":\n",
    "        class_name=\"农业\"\n",
    "    text_one_news=obatin_news_from_csv(i)\n",
    "    text_one_news=find_title(text_one_news)\n",
    "    text_one_news['分类']=[dict_label[class_name] for j in range(text_one_news.shape[0])]\n",
    "    \n",
    "    index_text_class=[\"微博正文\",\"标题\",\"分类\"]\n",
    "    text_one_news_1=text_one_news[index_text_class]\n",
    "    \n",
    "    text_one_news_1=text_one_news_1.drop_duplicates()\n",
    "    text_one_news_list.append(text_one_news_1)\n",
    "\n",
    "text_one_news_list2=[]\n",
    "text_one_news_list2.append(text_one_news_list[0].loc[len(text_one_news_list[0])-1000:len(text_one_news_list[0])])\n",
    "text_one_news_list2.append(text_one_news_list[1].loc[len(text_one_news_list[1])-1000:len(text_one_news_list[1])])\n",
    "text_one_news_list2.append(text_one_news_list[2].loc[len(text_one_news_list[2])-1000:len(text_one_news_list[2])])\n",
    "text_one_news_list2.append(text_one_news_list[3].loc[len(text_one_news_list[3])-1000:len(text_one_news_list[3])])\n",
    "text_one_news_list2.append(text_one_news_list[4].loc[1900:2900])\n",
    "\n",
    "for i in text_one_news_list:\n",
    "    print(i.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text_one_news_list2[0]\n",
    "for i in range(1,5):\n",
    "    text=text.append(text_one_news_list2[i])\n",
    "text=text.reset_index(drop=True)\n",
    "print(text.shape[0])\n",
    "text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start =time.clock()\n",
    "\n",
    "len_text=text.shape[0]\n",
    "\n",
    "text[\"微博正文(无标题切词后)\"]=[-99 for i in range(len_text)]\n",
    "text[\"标题(切词后)\"]=[-99 for i in range(len_text)]\n",
    "\n",
    "drop_na=[]\n",
    "\n",
    "progress = ProgressBar()\n",
    "\n",
    "for u in progress(range(0,len_text)):\n",
    "    cleaned_content=clean_text_content(text[\"微博正文\"][u])\n",
    "    text[\"微博正文(无标题切词后)\"][u]=\" \".join(cleaned_content)\n",
    "\n",
    "    cleaned_title=clean_text_content(text[\"标题\"][u],True)\n",
    "    text[\"标题(切词后)\"][u]=\" \".join(cleaned_title)\n",
    "\n",
    "    if pd.isnull(text.loc[u]).any():\n",
    "        drop_na.append(u)\n",
    "        print(u) \n",
    "        \n",
    "        \n",
    "    time.sleep(0.1)\n",
    "\n",
    "text=text.drop(drop_na)\n",
    "text=text.reset_index(drop=True)   \n",
    "\n",
    "text[\"微博正文(有标题切词后)\"]=[-99 for i in range(len_text)]\n",
    "progress = ProgressBar()\n",
    "\n",
    "for u in progress(range(0,len_text)):\n",
    "    text[\"微博正文(有标题切词后)\"][u]=text[\"标题(切词后)\"][u]+\" \"+text[\"微博正文(无标题切词后)\"][u]\n",
    "\n",
    "\n",
    "end = time.clock()\n",
    "\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath='./test_news_with_title_2.csv'\n",
    "text.to_csv(outputpath,sep=',',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv('./test_news_with_title_2.csv')\n",
    "a.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
